{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectSkin(image):\n",
    "    # Convert the image from BGR color space to YCrCb color space\n",
    "    ycrcb = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)\n",
    "\n",
    "    # Define the lower and upper boundaries of the skin color in the YCrCb color space\n",
    "    lower_skin = np.array([0, 133, 77], dtype=np.uint8)\n",
    "    upper_skin = np.array([255, 173, 127], dtype=np.uint8)\n",
    "\n",
    "    # Create a mask of the skin color in the YCrCb color space\n",
    "    mask = cv2.inRange(ycrcb, lower_skin, upper_skin)\n",
    "\n",
    "    # Apply the mask to the original image\n",
    "    skin = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "    return skin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTraceFromVidFile(VIDFOLDER, VERBOSE=0):\n",
    "    # extract RGB traces from vid file\n",
    "    # contact: yannick.benezeth@u-bourgogne.fr\n",
    "\n",
    "    # Parameters\n",
    "    DoSkinDetection = 0  # 0: ROI is the face, 1: ROI is the skin mask\n",
    "    vidFileName = 'vid.avi'\n",
    "    outFileName = 'rgbTraces'\n",
    "\n",
    "    # Create a cascade detector object.\n",
    "    faceDetector = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    # Create the point tracker object.\n",
    "    pointTracker = cv2.optflow.createTrackerByName('LK')\n",
    "    \n",
    "    if (os.path.exists(os.path.join(VIDFOLDER, f'{outFileName}.npy'))):\n",
    "        print('rgbTraces file exists, just skip getTraceFromVidFile()...')\n",
    "        return\n",
    "    \n",
    "    # Video Handler\n",
    "    vidObj = cv2.VideoCapture(os.path.join(VIDFOLDER, vidFileName))\n",
    "    fps = vidObj.get(cv2.CAP_PROP_FPS)\n",
    "    nbFrame = int(vidObj.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    rgbTraces = np.zeros((4, nbFrame))\n",
    "\n",
    "    ret, old_frame = vidObj.read()\n",
    "    old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "    oldPoints = cv2.goodFeaturesToTrack(old_gray, mask=None, maxCorners=100, qualityLevel=0.01,\n",
    "                                        minDistance=7, blockSize=7, useHarrisDetector=False)\n",
    "\n",
    "    numPts = 0\n",
    "    n = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, img = vidObj.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        n += 1\n",
    "        img_copy = img.copy()\n",
    "\n",
    "        # face localisation by detection then tracking\n",
    "        if numPts < 10:\n",
    "            # Detection mode.\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            faces = faceDetector.detectMultiScale(gray, 1.3, 5, minSize=(100, 100))\n",
    "            \n",
    "            if len(faces) == 0:\n",
    "                continue\n",
    "            \n",
    "            bbox = faces[0]  # Select first detected face\n",
    "\n",
    "            scaleX = 1\n",
    "            scaleY = 1.5\n",
    "            offsetX = 0.\n",
    "            offsetY = 0.1\n",
    "            bbox2 = [max(1, bbox[0] + bbox[2] * (offsetX + (1 - scaleX) / 2)),\n",
    "                     max(1, bbox[1] + bbox[3] * (offsetY + (1 - scaleY) / 2)),\n",
    "                     bbox[2] * scaleX, bbox[3] * scaleY]\n",
    "            bbox = bbox2\n",
    "\n",
    "            # initialize tracker\n",
    "            # Find corner points inside the detected region.\n",
    "            points = cv2.goodFeaturesToTrack(gray, mask=None, maxCorners=100, qualityLevel=0.01, \n",
    "                                             minDistance=7, blockSize=7, useHarrisDetector=False)\n",
    "\n",
    "            # Re-initialize the point tracker.\n",
    "            xyPoints = points.reshape(-1, 1, 2)\n",
    "            numPts = xyPoints.shape[0]\n",
    "            pointTracker.init(img, xyPoints)\n",
    "\n",
    "            # Save a copy of the points.\n",
    "            oldPoints = xyPoints.copy()\n",
    "\n",
    "            # Convert the rectangle represented as [x, y, w, h] into an\n",
    "            # M-by-2 matrix of [x,y] coordinates of the four corners. This\n",
    "            # is needed to be able to transform the bounding box to display\n",
    "            # the orientation of the face.\n",
    "            x, y, w, h = bbox\n",
    "            bboxPoints = np.array([[x, y], [x, y + h], [x + w, y + h], [x + w, y]])\n",
    "            # Crop the face region\n",
    "            x, y, w, h = bbox\n",
    "            bboxPoints = np.array([[x, y], [x, y + h], [x + w, y + h], [x + w, y]])\n",
    "            bboxPoints = np.matmul(np.concatenate((bboxPoints, np.ones((4, 1))), axis=1), M.T)\n",
    "            bboxPoints = bboxPoints[:, :2]\n",
    "            bbox = cv2.boundingRect(bboxPoints.astype(int))\n",
    "            if bbox[0] < 0:\n",
    "                bbox = (0, bbox[1], bbox[2] + bbox[0], bbox[3])\n",
    "            if bbox[1] < 0:\n",
    "                bbox = (bbox[0], 0, bbox[2], bbox[3])\n",
    "            if bbox[0] + bbox[2] >= img_copy.shape[1]:\n",
    "                bbox = (bbox[0], bbox[1], img_copy.shape[1] - bbox[0] - 1, bbox[3])\n",
    "            if bbox[1] + bbox[3] >= img_copy.shape[0]:\n",
    "                bbox = (bbox[0], bbox[1], bbox[2], img_copy.shape[0] - bbox[1] - 1)\n",
    "        \n",
    "            # Crop the face region\n",
    "            img_copy = img_copy[bbox[1]:bbox[1] + bbox[3], bbox[0]:bbox[0] + bbox[2]]\n",
    "            \n",
    "            # If we have a valid face region\n",
    "            if img_copy.shape[0] != 0 and img_copy.shape[1] != 0:\n",
    "                # Display the cropped face region\n",
    "                cv2.imshow('face region', img_copy)\n",
    "                cv2.waitKey(1)\n",
    "                # Convert the cropped face region to grayscale\n",
    "                gray = cv2.cvtColor(img_copy, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                # Skin detection\n",
    "                if DoSkinDetection:\n",
    "                    # Detect skin color\n",
    "                    skinMask = detectSkin(gray)\n",
    "                    # Extract only skin region\n",
    "                    img_copy = cv2.bitwise_and(img_copy, img_copy, mask=skinMask)\n",
    "                    # Display skin region\n",
    "                    cv2.imshow('skin region', img_copy)\n",
    "                    cv2.waitKey(1)\n",
    "                \n",
    "                # Feature detection and tracking\n",
    "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "                points, status, err = cv2.calcOpticalFlowPyrLK(old_gray, gray, oldPoints, None, **lk_params)\n",
    "                \n",
    "                # Update old_frame and oldPoints for the next iteration\n",
    "                old_gray = gray.copy()\n",
    "                oldPoints = points.reshape(-1, 1, 2)\n",
    "                \n",
    "                # Select good points\n",
    "                good_new = points[status == 1]\n",
    "                good_old = oldPoints[status == 1]\n",
    "                \n",
    "                # Get the RGB traces for this frame\n",
    "                R, G, B = getRGBTraces(good_new, good_old, img)\n",
    "                \n",
    "                # Store the RGB traces\n",
    "                rgbTraces[0, n - 1] = R\n",
    "                rgbTraces[1, n - 1] = G\n",
    "                rgbTraces[2, n - 1] = B\n",
    "                rgbTraces[3, n - 1] = n / fps\n",
    "                \n",
    "                # Update the previous points\n",
    "                oldPoints = good_new.reshape(-1, 1, 2)\n",
    "                numPts = oldPoints.shape[0]\n",
    "        \n",
    "        # Display the current frame number\n",
    "        if VERBOSE and (n % 100 == 0):\n",
    "            print(f'Frame {n}/{nbFrame} processed...')\n",
    "        \n",
    "        # Exit if the user presses the 'q' key\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the video object and close the windows\n",
    "    vidObj.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Save the RGB traces to file\n",
    "    np.save(os.path.join(VIDFOLDER, outFileName), rgbTraces)\n",
    "    print(f\"RGB traces saved to {os.path.join(VIDFOLDER, outFileName)}.npy\")\n",
    "\n",
    "    # Release the video and point tracker objects.\n",
    "    vidObj.release()\n",
    "    pointTracker = None\n",
    "    print('RGB traces extracted.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
